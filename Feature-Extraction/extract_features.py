#!/usr/bin/python

import os
import re
import numpy as np
import socket
import struct
import shutil
import subprocess
import json
import hashlib
import pandas as pd
import copy
import itertools


#json folder contains the reports got after executing samples in cuckoo and pcap folder contains the corresponding pcap files
pcap_dir = <path_to_pcap_folder_corresponding_to_json_folder>
tempfile = <path_to_temp_file>
jsonfoldername = <json_folder_name>
extract_pcap = <path_to_file_containing_network_features>
existing_reports  = [<path_to_json_folders>]
sign_file = <path_to_file_containing_signatures_used_as_features>
bin_file = <path_to_file_containing_API_bins>

fsbp_mal = <path_to_csv_to_save_extracted_features>
rows = []
one_gram = {}
one_gram_list = []
api_features = []
n = 2

def extract_one_gram(f):
	input_file=open(f, 'r')
	json_decode=json.load(input_file)
	if "behavior" in json_decode:
		behavior = json_decode["behavior"]
		if "apistats" in behavior:
			for process, apistats in behavior.get("apistats", {}).items():
				for k,v in apistats.items():
					if v >= 100:
						one_gram.setdefault(k, [])
						one_gram[k].append(v)

def find_list_file(f):
	api_list = ""	
	input_file=open(f, 'r')
	json_decode=json.load(input_file)
	if "behavior" in json_decode:
		behavior = json_decode["behavior"]
		if "processes" in behavior:
			for processes in behavior.get("processes", {}):
				if "calls" in processes:				
					for calls in processes.get("calls", {}):
						if "api" in calls:
							api = calls["api"]
							api_list += api + " "
	return api_list

#generate various combinations of one grams of API calls
def generate_features(num):
	featurelist = list(itertools.combinations(one_gram_list, num))
	for api in one_gram_list:
		templist = []
		for i in range(0, num):
			templist.append(api)
		featurelist.append(templist)
	return featurelist

def occurrences(string, sub):
    count = start = 0
    while True:
        start = string.find(sub, start) + 1
        if start > 0:
            count+=1
        else:
            return count

def load_file(fi):
	dict = {}	
	with open(fi) as f:
    		for line in f:
			if line.endswith('\n'):
            			line = line[:-1]			
			dict[line] = 0			

	return dict

def extract_signatures_val(f):
	features = load_file(sign_file)
	input_file=open(f, 'r')
	json_decode=json.load(input_file)
	if "signatures" in json_decode:	
		for signature in json_decode.get("signatures", {}):
			desc = signature["description"]
			if desc in features:
				features[desc] += 1
	return features


def extract_bins_val(f):
	features = load_file(bin_file)
	input_file=open(f, 'r')
	json_decode=json.load(input_file)

	if "behavior" in json_decode:
		behavior = json_decode["behavior"]
		if "processes" in behavior:
			for processes in behavior.get("processes", {}):
				if "calls" in processes:				
					for calls in processes.get("calls", {}):
						if "category" in calls:
							cat = calls["category"]
							features[cat] += 1
	return features


def extract_process_val(f):
	features = {}
	features['process_count'] = 0
	features['dropped_files'] = 0
	count  = 0
	
	input_file=open(f, 'r')
	json_decode=json.load(input_file)
	
	if "behavior" in json_decode:
		behavior = json_decode["behavior"]
		if "processes" in behavior:
			for processes in behavior.get("processes", {}):
				count += 1
			features['process_count'] = count
	
	count = 0	
	if "metadata" in json_decode:
		metadata = json_decode["metadata"]
		if "output" in metadata:
			output = metadata["output"]
			if "dropped" in output:
				for files in output.get("dropped", {}):
					count += 1
				features['dropped_files'] = count
			
	return features 




def http_info(f, root):
	bashline = "tshark -q -r" + " " + f + " " +"-Y http -z http,tree >" + " " + tempfile
	#print bashline
	os.system(bashline)
	tsharkOutput = open(tempfile, "r")
	
	for line in tsharkOutput:
	    if re.match("(.*)Total HTTP Packets(.*)", line):
	        parts = line.split() 
	    	features["Total HTTP Packets"] = parts[3]
	    elif re.match("(.*)HTTP Request Packets(.*)", line):
	        parts = line.split() 
	    	features["HTTP Request Packets"] = parts[3]    	
	    elif re.match("(.*)SEARCH(.*)", line):
	        parts = line.split() 
	    	features["SEARCH"] = parts[1]
	    elif re.match("(.*)NOTIFY(.*)", line):
	        parts = line.split() 
	    	features["NOTIFY"] = parts[1]
	    elif re.match("(.*)GET(.*)", line):
	        parts = line.split() 
	    	features["GET"] = parts[1]
	    elif re.match("(.*)POST(.*)", line):
	        parts = line.split() 
	    	features["POST"] = parts[1]
	    elif re.match("(.*)HTTP Response Packets(.*)", line):
	        parts = line.split() 
	    	features["HTTP Response Packets"] = parts[3]
	    elif re.match("(.*)2xx: Success(.*)", line):
	        parts = line.split() 
	    	features["Success"] = parts[2]    	    	    	    	    	    	
	    elif re.match("(.*)5xx: Server Error(.*)", line):
	        parts = line.split() 
	    	features["Server Error"] = parts[3]
	    elif re.match("(.*)4xx: Client Error(.*)", line):
	        parts = line.split() 
	    	features["Client Error"] = parts[3]
	    elif re.match("(.*)3xx: Redirection(.*)", line):
	        parts = line.split() 
	    	features["Redirection"] = parts[2] 

def protocol_extract(line):
	pfeatures=[]
	for i in range(2):
		pfeatures.append(0)
	parts = line.split()
	frames = parts[1].split(":")
	noOfByte = parts[2].split(":")
	pfeatures[0] = frames[1]
	pfeatures[1] = noOfByte[1]
	return pfeatures
    
def protocol_info(f, root):    
	bashline = "tshark -q -r"  + " " + f + " " + "-z io,phs >" + " " + tempfile
	#print bashline
	os.system(bashline)
	tsharkOutput = open(tempfile, "r")


	for line in tsharkOutput:
	    if re.match("(.*)ip(.*)", line):
	    	plist = protocol_extract(line)
	    	features["ip"] = plist[0]
	    elif re.match("(.*)udp(.*)", line):
	    	plist = protocol_extract(line)
	    	features["udp"] = plist[0]
	    elif re.match("(.*)bootp(.*)", line):
	    	plist = protocol_extract(line)
	    	features["bootp"] = plist[0]
	    elif re.match("(.*) dns(.*)", line):
	    	plist = protocol_extract(line)
	    	features["dns"] = plist[0]
	    elif re.match("(.*)llmnr(.*)", line):
	    	plist = protocol_extract(line)
	    	features["llmnr"] = plist[0]
	    elif re.match("(.*) mdns(.*)", line):
	    	plist = protocol_extract(line)
	    	features["mdns"] = plist[0]
	    elif re.match("(.*)data(.*)", line):
	    	plist = protocol_extract(line)
	    	features["data"] = plist[0]
	    elif re.match("(.*)ssdp(.*)", line):
	    	plist = protocol_extract(line)
	    	features["ssdp"] = plist[0]
	    elif re.match("(.*)nbns(.*)", line):
	    	plist = protocol_extract(line)
	    	features["nbns"] = plist[0]
	    elif re.match("(.*)tcp ", line):
	    	plist = protocol_extract(line)
	    	features["tcp"] = plist[0]
	    elif re.match("(.*)ssl(.*)", line):
	    	plist = protocol_extract(line)
	    	features["ssl"] = plist[0]
	    elif re.match("(.*)icmp(.*)", line):
	    	plist = protocol_extract(line)
	    	features["icmp"] = plist[0]
	    elif re.match("(.*)igmp(.*)", line):
	    	plist = protocol_extract(line)
	    	features["igmp"] = plist[0]


def ip_extract(f, protocol, templist, root):
	bashline = "tshark -r"  + " " + f + " " + "-Y" + " " + protocol + " " + "-T fields -e ip.src | sort | uniq -c >" + " " + tempfile
	#print bashline
	os.system(bashline)
	tsharkOutput = open(tempfile, "r")
	for line in tsharkOutput:
		parts = line.split()
		templist.append(parts[0])

def log_entropy(x):
	arr = np.asarray(x, dtype=np.int)
	temp = np.sum(np.log(np.array(range(1,np.sum(arr)))))
	
	for i in arr:
		temp -= np.sum(np.log(np.array(range(1,i))))
	return temp		


def is_private_ip(ip):
        
        networks = [
            "0.0.0.0/8",
            "10.0.0.0/8",
            "100.64.0.0/10",
            "127.0.0.0/8",
            "169.254.0.0/16",
            "172.16.0.0/12",
            "192.0.0.0/24",
            "192.0.2.0/24",
            "192.88.99.0/24",
            "192.168.0.0/16",
            "198.18.0.0/15",
            "198.51.100.0/24",
            "203.0.113.0/24",
            "240.0.0.0/4",
            "255.255.255.255/32",
            "224.0.0.0/4"
        ]

        for network in networks:
            try:
                ipaddr = struct.unpack(">I", socket.inet_aton(ip))[0]

                netaddr, bits = network.split("/")

                network_low = struct.unpack(">I", socket.inet_aton(netaddr))[0]
                network_high = network_low | (1 << (32 - int(bits))) - 1

                if ipaddr <= network_high and ipaddr >= network_low:
                    return True
            except:
                continue

        return False

def count_public_ip(f, root):
	dest_iplist=[]
	public_ip,private_ip = [0 for _ in range(2)]
	bashline = "tshark -r"  + " " + f + " " + "-Y ip -T fields -e ip.dst | sort | uniq -c >" + " " + tempfile
	#print bashline
	os.system(bashline)
	tsharkOutput = open(tempfile, "r")
	for line in tsharkOutput:
		parts = line.split()

		if re.match("(.*),(.*)", parts[1]):
			subparts = parts[1].split(",")

			for subpart in subparts:
				#print subpart
				if subpart not in dest_iplist:
					dest_iplist.append(subpart)
		else:
			if parts[1] not in dest_iplist:
				#print parts[1]
				dest_iplist.append(parts[1])
	
	
	for ip in dest_iplist:	
		#print ip
		if not is_private_ip(str(ip)):
			public_ip += 1
		else:
			private_ip += 1
				
	
	if private_ip != 0:
		features["ratio public private"] = float(public_ip)/float(private_ip)
	features["private ip"] = private_ip		

#extract process related features
def extract_from_json(f):
	file  = open(f, 'r').read()
	features["dead host"] = file.count('category": "dead_host"')
	features["domain"] = file.count('"domain":')

if __name__== "__main__":
	'''	
	print "extracting one gram :: "	
	for direc in existing_reports:
		for root, dirs, files in os.walk(direc):
			for f in files:
				filename = os.path.join(root, f)
				print filename
				extract_one_gram(filename)

	for k, v in one_gram.items():
		print k, v
		one_gram_list.append(k)

	api_features = generate_features(n)
	print api_features
	
	'''	
	count = 0
	for root, dirs, files in os.walk(pcap_dir):
		for f in files:
			filename = os.path.join(root, f)
			count += 1
			print filename, count
			
			features = load_file(extract_pcap)
			iplist=[]
			tcp_iplist=[]
			http_iplist=[]
							
			http_info(f, root)
			protocol_info(f, root)    

			ip_extract(f, "ip", iplist, root)
			features["ip entropy"] = log_entropy(iplist)
			
			ip_extract(f, "tcp", tcp_iplist, root)
			features["tcp entropy"] = log_entropy(tcp_iplist)

			ip_extract(f, "http", http_iplist, root)			
			features["http entropy"] = log_entropy(http_iplist)

			count_public_ip(f, root)

			lastindex = filename.rfind('/')
			index = filename.rfind('.')			
			lastindex1 = filename[:lastindex].rfind('/')
			
			features['sha256'] = filename[lastindex+1: index]
			filen = filename[:lastindex1] + "/" +  jsonfoldername +  "/" + filename[lastindex+1: index]  + ".json"
			print filen
			if os.path.isfile(filen):
				extract_from_json(filen)
				#apilist = find_list_file(filen)
				#print apilist
				#value_features = {}
				#for ft in api_features:
				#	substr = ' '.join(ft)
					#print "SUBSTRING::" + substr				
				#	value_features[substr] = occurrences(apilist, substr)

				features_sign = extract_signatures_val(filen)
				features_bins = extract_bins_val(filen) 
				features_process = extract_process_val(filen)

				combined = dict(features_sign, **features_bins)
				combined.update(features_process)
				#combined.update(value_features)
				combined.update(features)
				
				rows.append(combined)

df = pd.DataFrame(rows)			
with open(fsbp_mal, 'a') as f:
	df.to_csv(f, index = False)
			
			
