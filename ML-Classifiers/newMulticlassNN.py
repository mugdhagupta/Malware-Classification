from keras.models import Sequential
from keras.layers import Dense, Dropout, BatchNormalization, Activation
from keras.layers.advanced_activations import LeakyReLU, PReLU
import numpy as np
import os
import pandas as pd
from hyperas.distributions import uniform
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import itertools
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
from matplotlib.pyplot import *
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, f1_score
from sklearn import metrics
from numpy import array
from sklearn.metrics import accuracy_score
from xgboost import plot_importance
from matplotlib import pyplot
from numpy import sort
from keras.utils.np_utils import to_categorical
from keras.utils import np_utils
seed = 7
np.random.seed(seed)


def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

csvfile_train = <path_to_train_data.csv> 
csvfile_test = <path_to_test_data.csv> 

traindata = pd.read_csv(csvfile_train)
testdata = pd.read_csv(csvfile_test)

traindata = traindata[traindata['family'] != "benign"]
testdata = testdata[testdata['family'] != "benign"]

traindata = traindata.sample(frac=1).reset_index(drop=True)

print traindata.shape
print testdata.shape

unique_family_list = ['worms', 'virus', 'trojandropper', 'trojandownloader', 'trojan', 'backdoor',  'pws', 'virtool']
lab_encoder = LabelEncoder().fit(unique_family_list)


def pre_process_data(data):
	list_family = []
	len_data = len(data)
	i=0

	for row in data.iterrows():
		i+=1
		if i % 500 == 0:
		    print("%s | %s rows" % (i, len_data))
		type_labelized = lab_encoder.transform([row[1].family])[0]
        	list_family.append(type_labelized)
	
	data = data.drop(['family', 'sha256', 'Attempts to modify Explorer settings to prevent file extensions from being displayed', 'llmnr', 'domain', 'tcp', 'Expresses interest in specific running processes', 'udp', 'Attempts to identify installed AV products by registry key', 'SEARCH', 'modify_security_center_warnings', 'Creates a slightly modified copy of itself', 'nbns', 'Attempts to modify Explorer settings to prevent hidden files from being displayed', 'ip entropy', 'Success', 'ratio public private', 'Total HTTP Packets', 'Queries the disk size which could be used to detect virtual machine with small fixed size or dynamic allocation', 'NOTIFY', 'Disables Windows Security features', 'ssdp', 'Repeatedly searches for a not-found process, you may want to run a web browser during analysis', 'tcp entropy', 'HTTP Request Packets', 'process_count', 'Executes one or more WMI queries which can be used to identify virtual machines', 'http entropy'], axis=1)
	
	feat_names = list(data)	
	list_X = data.values
	list_family = np.array(list_family)
	return list_X, list_family, feat_names


X_train, y_train, feat_names = pre_process_data(traindata)
X_test, y_test, feat_names = pre_process_data(testdata)
print feat_names
print X_train.shape
print y_train.shape
print X_test.shape
print y_test.shape

scale = StandardScaler(with_mean=0, with_std=1)
new_X_train = scale.fit_transform(X_train)
new_X_test = scale.transform(X_test)

model = Sequential()

model.add(Dense(50, input_dim=52, kernel_initializer="normal"))
model.add(PReLU(alpha_initializer='zero', weights=None))
model.add(Dropout(0.5))

model.add(Dense(8, kernel_initializer='normal'))
model.add(Activation('softmax'))

#encoded_train = to_categorical(y_train)
#encoded_test = to_categorical(y_test)

model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])
history = model.fit(new_X_train, y_train, epochs=2000, batch_size=len(new_X_train)/4, validation_split=0.15)

#scores = model.evaluate(new_X_test, y_test)
y_proba = model.predict(new_X_test)
y_classes = np.argmax(y_proba, axis=-1)
#print("%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))

print(history.history.keys())
# summarize history for accuracy
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

cnf_matrix = confusion_matrix(y_test, y_classes)
np.set_printoptions(precision=2)

# Plot non-normalized confusion matrix
plt.figure()
plot_confusion_matrix(cnf_matrix, classes=lab_encoder.inverse_transform(range(8)),
                      title='Confusion matrix, without normalization')

plt.show()

